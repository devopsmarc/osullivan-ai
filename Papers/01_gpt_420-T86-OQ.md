<h2 align="center">GPT and BERT in Prompt-Based Learning</h2>
<p align="center"><strong>M.Sc Marcello Barretto</strong></p>
<p align="center" style="font-size: 0.9em; margin-top: -10px;">Collège O'Sullivan de Québec</p>
<p align="center" style="font-size: 0.8em; margin-top: -10px; font-style: italic;">{mabarretto}@osullivan-quebec.qc.ca</p>
<h4 align="center" style="font-weight: bold; font-style: italic;">Abstract</h4>

<p style="text-align:justify; font-size:0.8em; font-style:italic;">This paper explores the transformative roles of GPT and BERT in the realm of prompt-based learning within artificial intelligence. GPT, known for its generative capabilities, excels in tasks like translation and creative writing through its effective use of system prompts. In contrast, BERT, with its bidirectional training, offers a profound understanding of context within text, making it ideal for comprehension-intensive tasks. The paper delves into the synergy between these models, demonstrating how their combined strengths enhance applications. It also underscores the significance of prompt engineering—a skill crucial for leveraging these models effectively. By illuminating the distinct yet complementary capabilities of GPT and BERT, the paper highlights their impact on advancing AI and extending the boundaries of machine-human interaction.</p>

#### 1. Introduction

<p style="text-align:justify;font-size;">In the rapidly evolving field of artificial intelligence (AI), large language models (LLMs) like GPT and BERT have revolutionized the way we interact with machines. These sophisticated AI models are trained to understand and generate text in a human-like manner, opening up new ways for applications ranging from automated customer service to advanced data analysis.</p>


#### 2. Understanding GPT

<p style="text-align:justify; font-size;">GPT, short for Generative Pretrained Transformer, is a LLM developed by OpenAI. This model is trained on a massive corpus of text from the internet and has demonstrated impressive capabilities in various tasks such as translation, question-answering, and even writing essays. A distinguishing feature of GPT is its system prompt - a piece of text that tells the model what to do.

For instance, if you want GPT to generate a story about a knight saving a dragon, you might use the prompt "Once upon a time, there was a brave knight who set out on a quest to save a dragon." GPT takes this prompt and generates subsequent text that continues the story in a coherent and contextually appropriate manner.</p>

#### 3. BERT and GPT

<p style="text-align:justify; font-size;">While GPT makes waves with its generative capabilities, BERT (Bidirectional Encoder Representations from Transformers) stands as another significant development in the AI landscape. Unlike GPT which generates new text based on prompts, BERT's strength lies in understanding the context of words within a given piece of text.

BERT's bidirectional training, where it learns from both preceding and following words in a sentence, enables it to gain a deeper understanding of language. This feature makes BERT incredibly effective at tasks requiring comprehension of nuanced text, such as sentiment analysis or question answering.

Despite their different strengths, GPT and BERT are not mutually exclusive. In fact, their combined powers can be harnessed in areas like text summarization or classification. For instance, BERT's context understanding can be used to identify key parts of input text, which GPT can then use to generate concise summaries or accurate classifications.</p>


#### 4.System Prompts

<p style="text-align:justify; font-size;">System prompts act as instructions guiding these models' actions. For example, if you want to use GPT for summarizing an article, you might start with a prompt like "Summarize the following article:". Similarly, if you want to classify texts using these models, your prompt could be along the lines of "Classify this piece of feedback as positive or negative:".

However, crafting effective prompts is an art. It requires understanding not only what these models can do but also how they interpret different instructions. This process is known as prompt engineering and plays a crucial role in harnessing the power of these LLMs effectively.</p>


#### 5. Conclusion

In conclusion, both GPT with its generative prowess and BERT with its contextual understanding are undeniably transformative forces in AI. By leveraging system prompts intelligently, we can guide these models to perform tasks ranging from creative writing to complex data analysis. As we continue exploring their possibilities and pushing their boundaries, these models promise a future where AI's potential is limited only by our imagination. [^1^] [^2^] [^3^] [^4^] [^5^].

#### 6. BibTeX

<p style="font-size: 0.7em; margin-top: -10px;">
- - - </p>

<p style="font-size: 0.7em; margin-top: -10px;">
@misc{Collège O'Sullivan de Québec,</p>
<p style="font-size: 0.7em; margin-top: -10px;">
  author = {Marcello Barretto},</p>
<p style="font-size: 0.7em; margin-top: -10px;">
  title = {NLP, 2024},</p>
<p style="font-size: 0.7em; margin-top: -10px;">
  howpublished = "Collège O'Sullivan - e.Campus",</p>
<p style="font-size: 0.7em; margin-top: -10px;">
  year = {2024},</p>
<p style="font-size: 0.7em; margin-top: -10px;">
  note = "[GitHub Online; 420-T85-OQ]"}</p>

<p style="font-size: 0.7em; margin-top: -10px;">
- - - </p>

#### 7. References

[^1^]: [What gpt knows about who is who](https://arxiv.org/abs/2205.07407)
[^2^]: [Memory-assisted prompt editing to improve gpt-3 after deployment](https://arxiv.org/abs/2201.06009)
[^3^]: [Recent progress on text summarisation based on bert and gpt](https://link.springer.com/chapter/10.1007/978-3-031-40292-0_19)
[^4^]: [Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification](https://link.springer.com/chapter/10.1007/978-3-031-35320-8_1)
[^5^]: [Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models](https://www.tandfonline.com/doi/abs/10.1080/15391523.2022.2142872)




